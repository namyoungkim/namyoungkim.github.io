---
slug: probability-distance-part1
title: 분포 거리 측정 (Part 1) - 정보이론 기초와 KL Divergence
authors: namyoungkim
tags: [data, statistics, mathematics, machine-learning]
---

> KL Divergence를 정보이론 기초부터 이해하기. Self-Information, Entropy, Cross-Entropy의 관계를 통해 KL Divergence가 왜 이런 형태인지 직관적으로 설명합니다.

## 들어가며

머신러닝을 하다 보면 "두 확률 분포가 얼마나 다른가?"를 측정해야 하는 상황을 자주 만납니다. 모델이 예측한 분포와 실제 데이터 분포의 차이, VAE에서 latent space의 분포와 prior의 차이, 또는 시간에 따른 데이터 분포의 변화(drift) 감지까지. 이 모든 상황에서 **KL Divergence**라는 개념이 등장합니다.

이번 글에서는 KL Divergence가 "무엇"인지보다 **"왜"** 이런 형태를 갖게 되었는지에 초점을 맞춥니다. 정보이론의 기초 개념부터 차근차근 쌓아올려 보겠습니다.

<!-- truncate -->

---

## 1. 정보량 (Self-Information)

### 놀라움을 숫자로 표현하기

친구가 "오늘 해가 떴어"라고 말하면 별로 놀랍지 않습니다. 하지만 "오늘 서울에 눈이 2m 왔어"라고 하면 깜짝 놀라겠죠. 정보이론에서는 이 **"놀라움"의 정도**를 수학적으로 정의합니다.

$$I(x) = -\log P(x)$$

- $P(x)$: 사건 x가 발생할 확률
- $I(x)$: 사건 x의 정보량 (단위: bits 또는 nats)

### 왜 로그인가?

로그 함수를 사용하는 데는 합리적인 이유가 있습니다.

**첫째, 확률과 놀라움은 반비례합니다.**

```
확률 P(x)     정보량 I(x)
──────────────────────────
  1.0    →      0        (확실한 일 = 놀랍지 않음)
  0.5    →      1 bit    (동전 던지기)
  0.25   →      2 bits   (더 놀라움)
  0.125  →      3 bits   (훨씬 놀라움)
```

**둘째, 독립 사건의 정보량은 더해집니다.**

동전을 두 번 던져 둘 다 앞면이 나올 확률은 0.5 × 0.5 = 0.25입니다. 직관적으로 "두 번 다 앞면"의 놀라움은 "한 번 앞면"의 놀라움 두 배여야 합니다.

$$I(x \cap y) = -\log(P(x) \cdot P(y)) = -\log P(x) - \log P(y) = I(x) + I(y)$$

로그 덕분에 곱셈이 덧셈으로 변환되어 직관과 일치합니다.

### 시각화

```
정보량 I(x)
    ↑
  3 │        *
    │         *
  2 │          *
    │            *
  1 │              *
    │                  *  *  *
  0 └────────────────────────→ 확률 P(x)
    0    0.25   0.5   0.75   1

    확률이 낮을수록 정보량이 높다
```

---

## 2. 엔트로피 (Entropy)

### 평균적인 놀라움

개별 사건의 정보량을 알았으니, 이제 **분포 전체의 불확실성**을 측정할 수 있습니다. 엔트로피는 정보량의 기댓값입니다.

$$H(P) = \mathbb{E}_{x \sim P}[I(x)] = -\sum_x P(x) \log P(x)$$

### 예시: 동전 던지기

**공정한 동전** (앞면 50%, 뒷면 50%):

$$H = -[0.5 \log_2(0.5) + 0.5 \log_2(0.5)] = -[0.5 \times (-1) + 0.5 \times (-1)] = 1 \text{ bit}$$

**편향된 동전** (앞면 90%, 뒷면 10%):

$$H = -[0.9 \log_2(0.9) + 0.1 \log_2(0.1)] \approx 0.47 \text{ bits}$$

공정한 동전의 엔트로피가 더 높습니다. 결과를 예측하기 어려우니까요.

### 이진 엔트로피 함수

앞면 확률이 p인 동전의 엔트로피를 그래프로 그리면:

```
H(p)
    ↑
1.0 │        * * *
    │      *       *
0.8 │     *         *
    │    *           *
0.6 │   *             *
    │  *               *
0.4 │ *                 *
    │*                   *
0.0 └─────────────────────→ p
    0    0.25  0.5  0.75   1

    p = 0.5에서 최대 (가장 불확실)
    p = 0 또는 1에서 최소 (확실한 결과)
```

**핵심 직관**: 엔트로피는 "이 분포에서 샘플을 뽑았을 때 평균적으로 얼마나 놀라는가"를 측정합니다.

---

## 3. 교차 엔트로피 (Cross-Entropy)

### 잘못된 모델로 인코딩하기

이제 핵심적인 상황을 생각해봅시다. **실제 분포는 P인데, Q라는 다른 분포를 기반으로 메시지를 인코딩**한다면 얼마나 비효율적일까요?

$$H(P, Q) = -\sum_x P(x) \log Q(x) = \mathbb{E}_{x \sim P}[-\log Q(x)]$$

- 실제로 사건은 P를 따라 발생합니다 ($x \sim P$)
- 하지만 인코딩은 Q를 기준으로 합니다 ($-\log Q(x)$)

### 왜 이게 중요한가?

정보이론에서 최적의 인코딩은 자주 발생하는 사건에 짧은 코드를, 드문 사건에 긴 코드를 할당합니다. 실제 분포 P에 맞춰 인코딩하면 평균 코드 길이가 H(P)가 됩니다. 하지만 잘못된 분포 Q에 맞춰 인코딩하면 평균 코드 길이가 H(P, Q)가 되고, 이는 항상 H(P) 이상입니다.

### ML에서의 Cross-Entropy Loss

분류 문제에서 흔히 사용하는 Cross-Entropy Loss가 바로 이것입니다.

```
실제 레이블 (one-hot):  P = [0, 1, 0]  (클래스 2가 정답)
모델 예측 (softmax):    Q = [0.1, 0.7, 0.2]

H(P, Q) = -[0×log(0.1) + 1×log(0.7) + 0×log(0.2)]
        = -log(0.7)
        ≈ 0.36
```

모델이 정답에 높은 확률을 부여할수록 Cross-Entropy가 낮아집니다.

---

## 4. KL Divergence: 추가 비용의 측정

### 드디어 KL Divergence

이제 모든 재료가 준비되었습니다. KL Divergence는 다음과 같이 정의됩니다:

$$D_{KL}(P \| Q) = H(P, Q) - H(P) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$

### 의미 해석

```
┌─────────────────────────────────────────┐
│          Cross-Entropy H(P,Q)           │
│  ┌───────────────┬───────────────────┐  │
│  │  Entropy H(P) │  KL Divergence    │  │
│  │  (min cost)   │  (extra cost)     │  │
│  │               │                   │  │
│  │  "optimal     │  "waste from      │  │
│  │   encoding"   │   using Q"        │  │
│  └───────────────┴───────────────────┘  │
└─────────────────────────────────────────┘
```

- **Entropy H(P)**: 최소 비용 - P에 맞춘 최적 인코딩
- **KL Divergence**: 추가 비용 - Q를 사용해서 발생한 낭비

- **H(P)**: P 분포에 완벽히 맞춘 인코딩의 평균 비용 (이론적 최소)
- **H(P, Q)**: Q 분포에 맞춘 인코딩으로 P 분포 데이터를 처리할 때의 평균 비용
- **D_KL(P||Q)**: Q를 사용함으로써 발생하는 **추가 비용** (낭비)

### 핵심 성질

**1. 항상 0 이상**: $D_{KL}(P \| Q) \geq 0$

- P = Q일 때만 정확히 0
- 이를 **Gibbs' inequality**라고 합니다

**2. 비대칭**: $D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$

- "P를 Q로 근사"와 "Q를 P로 근사"는 다른 문제입니다
- 이 비대칭성이 Part 2에서 다룰 중요한 주제입니다

**3. 범위**: $[0, \infty)$

- Q(x) = 0인 지점에서 P(x) > 0이면 무한대로 발산합니다

---

## 5. 구체적 예시: 날씨 예측 모델

### 상황 설정

어떤 마을의 실제 날씨 분포 P와 기상청 예측 모델 Q가 있습니다.

| 날씨 | 실제 P | 모델 Q |
|------|--------|--------|
| 맑음 | 0.70 | 0.50 |
| 비 | 0.20 | 0.30 |
| 눈 | 0.10 | 0.20 |

### Step 1: 실제 분포의 엔트로피 H(P)

$$H(P) = -[0.7 \log_2(0.7) + 0.2 \log_2(0.2) + 0.1 \log_2(0.1)]$$

```
0.7 × (-0.515) = -0.361
0.2 × (-2.322) = -0.464
0.1 × (-3.322) = -0.332
──────────────────────
H(P) = 1.157 bits
```

이것이 날씨 정보를 전달하는 데 필요한 **이론적 최소 비용**입니다.

### Step 2: 교차 엔트로피 H(P, Q)

$$H(P, Q) = -[0.7 \log_2(0.5) + 0.2 \log_2(0.3) + 0.1 \log_2(0.2)]$$

```
0.7 × (-1.000) = -0.700
0.2 × (-1.737) = -0.347
0.1 × (-2.322) = -0.232
──────────────────────
H(P, Q) = 1.279 bits
```

기상청 모델 Q를 기준으로 인코딩하면 **평균 1.279 bits**가 필요합니다.

### Step 3: KL Divergence

$$D_{KL}(P \| Q) = H(P, Q) - H(P) = 1.279 - 1.157 = 0.122 \text{ bits}$$

### 해석

기상청 모델 Q를 사용하면, 최적의 경우보다 **메시지당 평균 0.122 bits를 낭비**합니다.

모델이 "맑음"의 확률을 과소평가(0.7 → 0.5)하고, "눈"의 확률을 과대평가(0.1 → 0.2)하기 때문에 실제 날씨 패턴을 효율적으로 표현하지 못하는 것입니다.

---

## 6. KL Divergence의 주의사항

### 비대칭성의 영향

$D_{KL}(P \| Q)$와 $D_{KL}(Q \| P)$는 다른 값을 가집니다.

```
D_KL(P || Q): "P에서 샘플링하고, Q로 평가"
              → P가 있는 곳에서 Q가 작으면 큰 페널티

D_KL(Q || P): "Q에서 샘플링하고, P로 평가"
              → Q가 있는 곳에서 P가 작으면 큰 페널티
```

이 비대칭성이 ML에서 어떤 행동 차이를 만드는지는 Part 2에서 자세히 다룹니다.

### Zero 확률 문제

Q(x) = 0인 지점에서 P(x) > 0이면:

$$P(x) \log \frac{P(x)}{Q(x)} = P(x) \log \frac{P(x)}{0} = \infty$$

실무에서는 이를 피하기 위해 smoothing을 적용합니다:

```pseudo
function stable_kl(P, Q, epsilon=1e-10):
    Q_smooth = Q + epsilon
    Q_smooth = Q_smooth / sum(Q_smooth)  # 재정규화

    kl = 0
    for each x where P(x) > 0:
        kl += P(x) * log(P(x) / Q_smooth(x))

    return kl
```

---

## 7. 정리

### 개념 간의 관계

```
Self-Information: I(x) = -log P(x)
        ↓
        ↓ expectation
        ↓
Entropy: H(P) = E_P[I(x)] = E_P[-log P(x)]
        ↓
        ↓ evaluate with different dist
        ↓
Cross-Entropy: H(P,Q) = E_P[-log Q(x)]
        ↓
        ↓ compute difference
        ↓
KL Divergence: D_KL(P||Q) = H(P,Q) - H(P)
```

- Self-Information → Entropy: 기댓값 계산
- Entropy → Cross-Entropy: 다른 분포 Q로 평가
- Cross-Entropy → KL Divergence: 차이 계산

### 핵심 요약

| 개념 | 의미 | 수식 |
|------|------|------|
| 정보량 | 사건의 놀라움 | $-\log P(x)$ |
| 엔트로피 | 분포의 불확실성 | $\mathbb{E}_P[-\log P(x)]$ |
| 교차 엔트로피 | 잘못된 모델의 비용 | $\mathbb{E}_P[-\log Q(x)]$ |
| KL Divergence | 추가 비용 (낭비) | $H(P,Q) - H(P)$ |

### 다음 편 예고

KL Divergence의 비대칭성은 단순한 수학적 성질이 아닙니다. ML 모델 학습에서 **완전히 다른 행동**을 유발합니다. Forward KL과 Reverse KL이 각각 어떤 상황에 적합한지, 그리고 VAE에서 왜 특정 방향의 KL을 사용하는지 Part 2에서 알아보겠습니다.

---

## 참고 자료

- Cover, T. M., & Thomas, J. A. "Elements of Information Theory"
- Bishop, C. M. "Pattern Recognition and Machine Learning" Chapter 1.6
